input {
    beats {
        port => 5044 ## Reads Filebeat outputs
    }
}

filter {

    if "starling-logs" in [tags] {
        # Format Starling Wifi logs.

        grok {
            match => { "message" => [
               "\[%{TIMESTAMP_ISO8601:timestamp}\] %{DATA:env}\.%{DATA:severity}: %{GREEDYDATA:context}", # For standard logs.
               "\[%{DATA:prevexception}\] %{GREEDYDATA:context}" # For logs starting with [previous exception]
           ]}
        }

        # Some laravel logs starts like [previous exception] ... without timestamps or anything.
        # For that case we dont touch @timestamp so that, Elastic can stack them as normal logs with others.
        # Note that: @timestamp in those logs corresponds to moment logstash pushed them to elastic, not actual log timestamp.
        if ![prevexception] {
            # Rename logstash generated @timestamp as read_timestamp to keep push-time in log.
            mutate {
                rename => { "@timestamp" => "read_timestamp" }
            }

            # Make sure timestamp is formatted correctly.
            date {
                match => [ "timestamp", "YYYY-MM-dd HH:mm:ss" ]
                remove_field => "timestamp"
            }
        }

        # Remove message field after parsing it.
        # Add index prefix.
        mutate {
            add_field => { "logtype" => "starling-logs"}
            remove_field => [ "message" ]
        }

    } else if "mysql-logs" in [tags] {

        # Format MySQL error logs.

        grok {
            match => { "message" => [
                "%{DATA:timestamp} %{NUMBER:tid} \[%{DATA:severity}\] %{GREEDYDATA:context}", # For standard logs.
                "%{GREEDYDATA:context}" # For logs not in standard format.
            ]}
        }

        if [timestamp] {
            # Note: Check explanations on starling-wifi filter.
            mutate {
                rename => { "@timestamp" => "read_timestamp" }
            }

            # MySQL has messy timestamp formats in the logs. Before 12 o'clock, hour does not contain zero-padding on the left.
            # That's why we use two matching to make sure.
            date {
                match => [ "timestamp", "ISO8601", "YYYY-MM-dd HH:mm:ss", "YYYY-MM-dd  H:mm:ss" ]
                remove_field => "timestamp"
            }
        }

        # Remove message field after parsing it.
        # Add index prefix.
        mutate {
            add_field => { "logtype" => "mysql-logs"}
            remove_field => [ "message" ]
        }

    } else if "nginx-logs" in [tags] {

        # Format Nginx error logs.

        grok {
            match => { "message" => "%{DATA:timestamp} \[%{DATA:severity}\] %{NUMBER:pid}#%{NUMBER:tid}: (\*%{NUMBER:connectionId} )?%{GREEDYDATA:context}" }
        }

        # Note: Check explanations on starling-wifi filter.
        mutate {
            rename => { "@timestamp" => "read_timestamp" }
        }

        # Make sure to format timestamp correctly.
        date {
            match => [ "timestamp", "YYYY/MM/dd H:m:s" ]
            remove_field => "timestamp"
        }

        # Remove message field after parsing it.
        # Add index prefix.
        mutate {
            add_field => { "logtype" => "nginx-logs"}
            remove_field => [ "message" ]
        }

    } else if "radius-logs" in [tags] {

        # Format FreeRadius logs.

        grok {
            patterns_dir => ["/usr/share/logstash/pipeline/patterns"]
            match => { "message" => [
                "%{FREERADIUS_DATE:timestamp} : %{DATA:severity}: rlm_sql \(%{DATA:db_name}\): %{GREEDYDATA:context}",
                "%{FREERADIUS_DATE:timestamp} : %{DATA:severity}: %{GREEDYDATA:context}"
            ]}
        }

        # Note: Check explanations on starling-wifi filter.
        mutate {
            rename => { "@timestamp" => "read_timestamp" }
        }

        # Make sure to format timestamp correctly.
        date {
            match => [ "timestamp", "EEE MMM dd HH:mm:ss yyyy", "EEE MMM  d HH:mm:ss yyyy" ]
            remove_field => "timestamp"
        }

        mutate {
            add_field => { "logtype" => "radius-logs"}
            remove_field => [ "message" ]
        }

    } else if "raddact-logs" in [tags] {

        # Format radactt logs.

        # Pull off the timestamp at the start of the
        # detail record. Note there may be additional data
        # after it that has been added by the local admin,
        # so stop at a newline OR a tab.
        grok {
            match => [ "message", "^(?<timestamp>[^\n\t]+)[\n\t]" ]
        }

        # Create the @timestamp field.
        date {
            match => [ "timestamp", "EEE MMM dd HH:mm:ss yyyy", "EEE MMM  d HH:mm:ss yyyy" ]
        }

        # Split the attributes and values into fields.
        # This is the bulk of processing that adds all of
        # the RADIUS attributes as elasticsearch fields.
        kv {
#            field_split => "\n"
            source => "message"
#            trim_value => "\n"
#            trim_key => "\t "
            field_split => "\n"
            value_split => "="
            trim_key => " \t\""
            trim_value => "\" "

        }

        mutate {
            add_field => { "logtype" => "radactt-logs"}
            #            remove_field => [ "message" ]
        }

        # Now we try and add some useful additional
        # information. If certain fields can be broken
        # down into components then do that here and add
        # the data as sub-fields. For example,
        # Called-Station-Id might be able to be broken
        # down to Called-Station-Id_mac and Called-Station-Id_ssid
        # on some wireless systems, or to _ip and _port
        # with a VPN.

        # Multiple calls to grok otherwise it can stop
        # processing once it has matched one field, but
        # e.g. you want to pull both IP and port out of
        # the same field in two different regex's.

        # Pull out some IP addresses as field_ip:

#        grok {
#            break_on_match => false
#            tag_on_failure => ['radfail_1']
#            match => [
#                "Framed-IP-Address", "^(?<Framed-IP-Address_ip>\d+\.\d+\.\d+\.\d+$)",
#                "NAS-IP-Address", "^(?<NAS-IP-Address_ip>\d+\.\d+\.\d+\.\d+$)",
#                "Calling-Station-Id", "^(?<Calling-Station-Id_ip>\d+\.\d+\.\d+\.\d+)",
#                "Called-Station-Id", "^(?<Called-Station-Id_ip>\d+\.\d+\.\d+\.\d+)"
#            ]
#        }

        # Split User-Name, Operator-Name, and pull out
        # some IP ports if they are there:
#
#        grok {
#            break_on_match => false
#            tag_on_failure => ['radfail_2']
#            match => [
#                "User-Name", "^(?<User-Name_username>[^@]+)?(?:@(?<User-Name_realm>[^@]+))$",
#                "Operator-Name", "^(?<Operator-Name_id>.)(?<Operator-Name_value>.+)$",
#                "Calling-Station-Id", "\[(?<Calling-Station-Id_port>\d+)\]$",
#                "Called-Station-Id", "\[(?<Called-Station-Id_port>\d+)\]$"
#            ]
#        }

        # Extract MAC addresses (and SSIDs if there).
        # MAC address matching here is lazy, but should be
        # good enough.

#        grok {
#            break_on_match => false
#            tag_on_failure => ['radfail_3']
#            match => [
#                "Calling-Station-Id", "^(?<Calling-Station-Id_mac>[a-fA-F0-9:-]{17})$",
#                "Calling-Station-Id", "^(?<Calling-Station-Id_mac>[a-fA-F0-9\.]{14})$",
#                "Calling-Station-Id", "^(?<Calling-Station-Id_mac>[a-fA-F0-9]{12})$",
#
#                "Called-Station-Id", "^(?<Called-Station-Id_mac>[a-fA-F0-9:-]{17})(?::(?<Called-Station-Id_ssid>.*))?$",
#                "Called-Station-Id", "^(?<Called-Station-Id_mac>[a-fA-F0-9\.]{14})(?::(?<Called-Station-Id_ssid>.*))?$",
#                "Called-Station-Id", "^(?<Called-Station-Id_mac>[a-fA-F0-9]{12})(?::(?<Called-Station-Id_ssid>.*))?$"
#            ]
#        }
#
#        if ([Acct-Input-Octets]) {
#            ruby {
#                code => "event['Acct-Input-Octets_long'] =
#                    event['Acct-Input-Octets'].to_i + ( event['Acct-Input-Gigawords'] ? (event['Acct-Input-Gigawords'].to_i * (2**32)) : 0)"
#                }
#        }
#
#        if ([Acct-Output-Octets]) {
#            ruby {
#                code => "event['Acct-Output-Octets_long'] =
#                    event['Acct-Output-Octets'].to_i + ( event['Acct-Output-Gigawords'] ? (event['Acct-Output-Gigawords'].to_i * (2**32)) : 0)"
#                }
#        }

#        mutate {
#            add_field => { "logtype" => "radactt-logs"}
##            remove_field => [ "message" ]
#        }
    }
}

output {

    stdout {
        codec => rubydebug
    }

    elasticsearch {
        hosts => "elasticsearch:9200"
        manage_template => false
        index => "%{logtype}-%{+YYYY.MM.dd}"
#        user => "elastic"
#        password => "changeme"
    }
}